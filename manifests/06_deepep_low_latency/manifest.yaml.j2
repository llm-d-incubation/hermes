---
apiVersion: v1
kind: Service
metadata:
  name: deepep-lowlatency-master-{{ test_id }}
  labels:
    app: deepep-lowlatency-test
    test-id: "{{ test_id }}"
spec:
  clusterIP: None
  selector:
    app: deepep-lowlatency-test
    role: master
    test-id: "{{ test_id }}"
  ports:
  - port: 29500
    name: dist
---
apiVersion: batch/v1
kind: Job
metadata:
  name: deepep-lowlatency-master-{{ test_id }}
  labels:
    app: deepep-lowlatency-test
    role: master
    test-id: "{{ test_id }}"
spec:
  template:
    metadata:
      labels:
        app: deepep-lowlatency-test
        role: master
        test-id: "{{ test_id }}"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/hostname: "{{ server_node.name }}"
      containers:
      - name: deepep-lowlatency-master
        image: {{ image }}
        command: ["/bin/bash"]
        args:
        - -c
        - |
          set -e
          echo "Starting DeepEP low latency test (MASTER) on node {{ server_node.name }}"

          echo "GPU information:"
          nvidia-smi -L

          GPU_COUNT=$(nvidia-smi -L | wc -l)
          echo "Detected $GPU_COUNT GPUs"

          echo "Cloning DeepEP repository..."
          cd /tmp
          git clone https://github.com/deepseek-ai/DeepEP || echo "Repository already exists"
          cd DeepEP

          TOTAL_GPUS=$((GPU_COUNT * 2))
          echo "Running DeepEP low latency test with $TOTAL_GPUS total GPUs (rank 0-$((GPU_COUNT-1)))"

          export MASTER_ADDR=deepep-lowlatency-master-{{ test_id }}
          export MASTER_PORT=29500
          export WORLD_SIZE=$TOTAL_GPUS
          export RANK=0

          python tests/test_low_latency.py --num-processes $GPU_COUNT --num-tokens 512 --hidden 1024 --num-topk 2 --num-experts 32

          echo "DeepEP low latency test completed successfully"
        resources:
          requests:
            {{ rdma_resource_type }}: "1"
            nvidia.com/gpu: "2"
            memory: 8Gi
            cpu: "4"
          limits:
            {{ rdma_resource_type }}: "1"
            nvidia.com/gpu: "2"
            memory: 16Gi
            cpu: "8"
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
---
apiVersion: batch/v1
kind: Job
metadata:
  name: deepep-lowlatency-worker-{{ test_id }}
  labels:
    app: deepep-lowlatency-test
    role: worker
    test-id: "{{ test_id }}"
spec:
  template:
    metadata:
      labels:
        app: deepep-lowlatency-test
        role: worker
        test-id: "{{ test_id }}"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/hostname: "{{ client_node.name }}"
      containers:
      - name: deepep-lowlatency-worker
        image: {{ image }}
        command: ["/bin/bash"]
        args:
        - -c
        - |
          set -e
          echo "Starting DeepEP low latency test (WORKER) on node {{ client_node.name }}"

          echo "GPU information:"
          nvidia-smi -L

          GPU_COUNT=$(nvidia-smi -L | wc -l)
          echo "Detected $GPU_COUNT GPUs"

          echo "Cloning DeepEP repository..."
          cd /tmp
          git clone https://github.com/deepseek-ai/DeepEP || echo "Repository already exists"
          cd DeepEP

          echo "Waiting for master to be ready..."
          until getent hosts deepep-lowlatency-master-{{ test_id }}; do
            echo "Waiting for master service..."
            sleep 2
          done
          sleep 5

          TOTAL_GPUS=$((GPU_COUNT * 2))
          echo "Running DeepEP low latency test with $TOTAL_GPUS total GPUs (rank $GPU_COUNT-$((TOTAL_GPUS-1)))"

          export MASTER_ADDR=deepep-lowlatency-master-{{ test_id }}
          export MASTER_PORT=29500
          export WORLD_SIZE=$TOTAL_GPUS
          export RANK=$GPU_COUNT

          python tests/test_low_latency.py --num-processes $GPU_COUNT --num-tokens 512 --hidden 1024 --num-topk 2 --num-experts 32

          echo "DeepEP low latency test completed successfully"
        resources:
          requests:
            {{ rdma_resource_type }}: "1"
            nvidia.com/gpu: "2"
            memory: 8Gi
            cpu: "4"
          limits:
            {{ rdma_resource_type }}: "1"
            nvidia.com/gpu: "2"
            memory: 16Gi
            cpu: "8"
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
