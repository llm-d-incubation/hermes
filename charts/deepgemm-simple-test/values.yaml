# Default values for deepgemm-simple-test
# These will be overridden by hermes select-nodes --format helm-values

# test configuration
testId: "manual-test"  # unique identifier for this test run
namespace: default  # kubernetes namespace for test resources
activeDeadlineSeconds: 300  # maximum time for jobs to complete (5 minutes)
ttlSecondsAfterFinished: 300  # auto-cleanup JobSet after completion (5 minutes)

# container image with DeepGEMM, PyTorch, and CUDA support
# this image must include:
# - DeepGEMM library (ai-dynamo/DeepGEMM)
# - PyTorch with CUDA support
# - CUDA/GPU drivers
image: ghcr.io/llm-d/llm-d-cuda-dev:sha-d58731d@sha256:ba067a81b28546650a5496c3093a21b249c3f0c60d0d305ddcd1907e632e6edd

# resource requests for GPU compute
resources:
  # GPU resource name (usually nvidia.com/gpu for NVIDIA GPUs)
  gpu: nvidia.com/gpu

  # resource requests (guaranteed allocation)
  requests:
    memory: 4Gi  # minimum memory
    cpu: "2"  # minimum CPU cores

  # resource limits (maximum allowed)
  limits:
    memory: 8Gi  # maximum memory
    cpu: "4"  # maximum CPU cores

# topology configuration - automatically injected by hermes select-nodes
# this section describes the cluster topology and selected node pair
topology:
  # list of nodes selected for the test (always 2 nodes)
  nodes:
    - name: node1  # first node
      gpus: 8  # number of GPUs on this node
      rank: 0  # rank in the test (0 = node1)
      topologyBlock: unknown  # platform-specific grouping (e.g., leafgroup, fabric domain, zone)
    - name: node2  # second node
      gpus: 8  # number of GPUs on this node
      rank: 1  # rank in the test (1 = node2)
      topologyBlock: unknown  # platform-specific grouping

  # aggregated topology summary
  summary:
    totalNodes: 2  # total nodes in test (always 2)
    totalGpus: 16  # total GPUs across all nodes
    gpusPerNode: 8  # average GPUs per node
    worldSize: 16  # total GPU count (used for distributed training context)

  # platform detected by hermes
  # options: CoreWeave, GKE, OpenShift, Generic
  platform: Generic

  # whether both nodes are in the same topology block
  # true = optimal placement for local fabric testing
  # false = cross-block testing (may have higher latency)
  allSameBlock: false
