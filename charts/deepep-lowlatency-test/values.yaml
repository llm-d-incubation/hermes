# Default values for deepep-lowlatency-test
# These will be overridden by hermes select-nodes --format helm-values

# test configuration
testId: "manual-test"  # unique identifier for this test run
namespace: default  # kubernetes namespace for test resources
activeDeadlineSeconds: 600  # maximum time for jobs to complete (10 minutes)

# container image with DeepEP, NCCL, NVSHMEM, UCX, and CUDA support
# this image must include:
# - DeepSeek DeepEP library (expert parallelism framework)
# - NCCL (NVIDIA Collective Communications Library)
# - NVSHMEM (NVIDIA Symmetric Memory)
# - UCX (Unified Communication X) for RDMA
# - CUDA/GPU drivers
# - PyTorch
image: ghcr.io/llm-d/llm-d-cuda-dev:sha-d58731d@sha256:ba067a81b28546650a5496c3093a21b249c3f0c60d0d305ddcd1907e632e6edd

# resource requests for RDMA and GPU
resources:
  # RDMA resource name (platform-specific)
  # examples: rdma/ib (InfiniBand), rdma/roce_gdr (RoCE), networking.gke.io.networks/rdma-vpc0
  rdma: rdma/ib

  # number of RDMA devices per pod (DeepEP requires 2 for dual-HCA config)
  rdmaQuantity: "2"

  # GPU resource name (usually nvidia.com/gpu for NVIDIA GPUs)
  gpu: nvidia.com/gpu

  # number of GPUs per pod (will be auto-detected from node info)
  gpuQuantity: "8"

  # resource requests (guaranteed allocation)
  requests:
    memory: 8Gi  # minimum memory
    cpu: "4"  # minimum CPU cores

  # resource limits (maximum allowed)
  limits:
    memory: 16Gi  # maximum memory
    cpu: "8"  # maximum CPU cores

# UCX (Unified Communication X) configuration for RDMA transport
ucx:
  # log verbosity: error, warn, info, debug, trace
  logLevel: info

  # comma-separated list of transports to use
  # - rc: reliable connection (IB native)
  # - ud: unreliable datagram (IB native)
  # - dc: dynamic connection (IB native)
  # - tcp: fallback for non-RDMA
  # - cuda_copy, cuda_ipc: GPU-GPU transfers
  # - gdr_copy: GPUDirect RDMA
  transports: rc,ud,dc,tcp,cuda_copy,cuda_ipc,gdr_copy

  # GID (Global Identifier) index for RDMA
  # - "0": default for InfiniBand
  # - "3": often required for RoCE v2
  gidIndex: "0"

# SR-IOV configuration (only for RoCE on OpenShift/SR-IOV platforms)
sriov:
  # whether to enable SR-IOV network attachments
  enabled: false

  # SR-IOV network name (e.g., "sriov-rdma-network")
  # leave empty to auto-detect or use multi-nic-compute
  network: ""

  # SR-IOV network resource (e.g., "openshift.io/sriov_rdma")
  # if empty, uses multi-nic-compute mode instead
  resource: ""

  # default interface name for SR-IOV VF (net1 for single, net1-0 for multi-nic)
  interface: "net1"

# NCCL configuration for collective operations
nccl:
  # debug level: VERSION, WARN, INFO, TRACE
  debug: "INFO"

  # socket interface for NCCL (will be set to sriov.interface)
  socketIfname: ""

# DeepEP test parameters
deepep:
  # number of tokens per test
  numTokens: 128

  # hidden dimension size
  hidden: 2048

  # number of top-k experts to select
  numTopk: 4

  # total number of experts in the MoE
  numExperts: 32

  # git tag/branch to checkout
  version: "v1.2.1"

# topology configuration - automatically injected by hermes select-nodes
# this section describes the cluster topology and selected node pair
topology:
  # list of nodes selected for the test (always 2 nodes for DeepEP)
  nodes:
    - name: node1  # first node (master rank 0)
      gpus: 8  # number of GPUs on this node
      rank: 0  # rank in the test (0 = master)
      topologyBlock: unknown  # platform-specific grouping (e.g., leafgroup, fabric domain, zone)
    - name: node2  # second node (worker rank 1)
      gpus: 8  # number of GPUs on this node
      rank: 1  # rank in the test (1 = worker)
      topologyBlock: unknown  # platform-specific grouping

  # aggregated topology summary
  summary:
    totalNodes: 2  # total nodes in test (always 2)
    totalGpus: 16  # total GPUs across all nodes
    gpusPerNode: 8  # average GPUs per node
    worldSize: 16  # total GPU count (used for distributed training context)

  # RDMA type detected on these nodes
  rdmaType: rdma/ib  # examples: rdma/ib, rdma/roce_gdr, networking.gke.io.networks/rdma-vpc0

  # platform detected by hermes
  # options: CoreWeave, GKE, OpenShift, Generic
  platform: Generic

  # whether both nodes are in the same topology block
  # true = optimal placement for local RDMA fabric testing
  # false = cross-block testing (may have higher latency)
  allSameBlock: false
